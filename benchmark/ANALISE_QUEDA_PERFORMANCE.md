# ðŸš¨ ANÃLISE: Queda de Performance no Benchmark v2.0

**Data:** 2025-10-31
**Problema:** Score caiu de 75.6 para 55.2 (-20.4 pontos)

---

## ðŸ“Š COMPARAÃ‡ÃƒO DE RESULTADOS

### Modelo: intfloat/multilingual-e5-base

| MÃ©trica | Benchmark Inicial | Benchmark v2.0 | VariaÃ§Ã£o |
|---------|------------------|----------------|----------|
| **Score Geral** | **75.6/100** âœ… | **55.2/100** âŒ | **-20.4** |
| **AcurÃ¡cia Top-5** | **87.5%** | **55.7%** | **-31.8%** |
| **AcurÃ¡cia Top-1** | **62.5%** | **36.4%** | **-26.1%** |
| **DistÃ¢ncia MÃ©dia** | 0.3387 | 0.3190 | -0.0197 âœ… |
| **Ground Truth** | 8 casos | 88 casos | +80 casos |
| **Tempo** | 4971s (83 min) | 3823s (64 min) | -19 min âœ… |

**Status:** âš ï¸ **REGRESSÃƒO CRÃTICA**

---

## ðŸ” CAUSAS IDENTIFICADAS

### 1. Ground Truth Mais Rigoroso âœ… (Esperado)

**AnÃ¡lise:** Expandir de 8 para 88 casos **torna o teste mais difÃ­cil**.

**Casos adicionados incluem:**
- âŒ Consultas genÃ©ricas: `"carro"` â†’ difÃ­cil classificar
- âŒ Consultas muito especÃ­ficas: `"smartphone samsung galaxy s21 128gb"`
- âŒ Erros ortogrÃ¡ficos: `"telefon celular"` (sem 'e')
- âŒ Consultas em inglÃªs: `"mobile phone"`

**Impacto estimado:** -5 a -10 pontos no score

**Veredicto:** âœ… **ESPERADO** - Ground truth mais realista

---

### 2. NormalizaÃ§Ã£o de Texto Agressiva âš ï¸

**CÃ³digo implementado:**
```python
def normalize_text_advanced(text):
    # Remove acentos
    text = unicodedata.normalize('NFKD', text)
    text = text.encode('ASCII', 'ignore').decode('ASCII')

    # Lowercase
    text = text.lower()

    # Remove pontuaÃ§Ã£o
    text = re.sub(r'[^\w\s-]', ' ', text)

    # Remove stopwords
    stopwords = {'de', 'da', 'do', 'em', 'na', 'no', 'para', 'com'}
    words = [w for w in text.split() if w not in stopwords]

    return ' '.join(words)
```

**Problema:** Pode estar removendo contexto importante!

**Exemplo:**
```
ANTES:
"NCM 0901.11.10: CafÃ© em grÃ£o nÃ£o descafeinado
CapÃ­tulo: CafÃ©, chÃ¡, mate e especiarias"

DEPOIS:
"ncm 0901 11 10 cafe grao nao descafeinado | capitulo cafe cha mate especiarias"
```

**Potenciais problemas:**
- âŒ Separador ` | ` pode confundir modelo
- âŒ RemoÃ§Ã£o de stopwords pode perder contexto (`"em grÃ£o"` â†’ `"grao"`)
- âŒ Quebra de nÃºmeros NCM (`0901.11.10` â†’ `0901 11 10`)

**Impacto estimado:** -5 a -10 pontos

**Veredicto:** âš ï¸ **SUSPEITO** - NormalizaÃ§Ã£o pode estar muito agressiva

---

### 3. Bug no Cache - Nome de Modelo Errado ðŸ›

**Problema detectado:**
```
Cache atual mostra:
  - SentenceTransformer: 12731 embeddings  ðŸ‘ˆ NOME GENÃ‰RICO ERRADO!

Deveria mostrar:
  - intfloat/multilingual-e5-base: 12731 embeddings
```

**CÃ³digo problemÃ¡tico:**
```python
# ANTES (bugado)
model_name = embedder._model_card_data.model_name if hasattr(embedder, '_model_card_data') else str(type(embedder).__name__)
#                                                                                                ^^^ Retorna "SentenceTransformer"
```

**ConsequÃªncia:** Cache estÃ¡ salvando embeddings de **modelos diferentes com mesmo nome**!

Isso pode causar:
- âœ… Cache hit em modelo errado
- âŒ Embeddings de um modelo sendo usado por outro
- âŒ Resultados inconsistentes

**Impacto estimado:** -5 a -15 pontos (SE houver mistura de modelos)

**Veredicto:** ðŸ› **BUG CRÃTICO** - Corrigido neste commit

---

## âœ… CORREÃ‡Ã•ES IMPLEMENTADAS

### 1. Progresso de Cache: 1000 â†’ 10000

**MudanÃ§a:**
```python
# ANTES
if (idx + 1) % 1000 == 0:
    print(f"  Salvos: {idx + 1}/{len(texts)}")

# DEPOIS
if (idx + 1) % 10000 == 0:
    print(f"  Salvos: {idx + 1}/{len(texts)}")
```

**BenefÃ­cio:** Menos poluiÃ§Ã£o visual, progresso mais limpo

---

### 2. Nome Correto do Modelo no Cache

**MudanÃ§a:**
```python
# ANTES (bugado)
model_name = embedder._model_card_data.model_name if hasattr(embedder, '_model_card_data') else str(type(embedder).__name__)

# DEPOIS (corrigido)
if hasattr(embedder, '_model_card_data') and hasattr(embedder._model_card_data, 'model_name'):
    model_name = embedder._model_card_data.model_name
elif hasattr(embedder, 'model_card_data') and hasattr(embedder.model_card_data, 'model_name'):
    model_name = embedder.model_card_data.model_name
elif hasattr(embedder, '_model_name'):
    model_name = embedder._model_name
else:
    # Fallback robusto
    if hasattr(embedder, '_modules') and '0' in embedder._modules:
        transformer = embedder._modules['0']
        if hasattr(transformer, 'auto_model'):
            model_name = transformer.auto_model.name_or_path
        else:
            model_name = "unknown_model"
```

**BenefÃ­cio:** Cache agora usa nome real do modelo

---

### 3. Debug de Nome do Modelo

**Adicionado:**
```python
# Debug: mostra nome do modelo usado no cache
if len(texts) > 1000:
    print(f"Cache: usando modelo '{model_name}'")
```

**BenefÃ­cio:** Visibilidade de qual modelo estÃ¡ sendo usado

---

### 4. Script para Limpar Cache Corrompido

**Criado:** `clear_cache.py`

**Uso:**
```bash
# Limpar todo o cache
python clear_cache.py

# Limpar apenas modelos especÃ­ficos
python clear_cache.py --model e5
```

---

## ðŸŽ¯ RECOMENDAÃ‡Ã•ES

### AÃ§Ã£o Imediata

1. **Limpar cache corrompido:**
   ```bash
   python clear_cache.py
   ```

2. **Re-executar benchmark:**
   ```bash
   python run_definitive_benchmark.py
   ```

3. **Comparar resultados:**
   - Se score voltar para ~75: bug do cache era a causa
   - Se continuar ~55: problema Ã© a normalizaÃ§Ã£o ou ground truth

---

### Teste A/B da NormalizaÃ§Ã£o

Se score continuar baixo, testar **sem normalizaÃ§Ã£o**:

```python
# Em data_loader.py, comentar normalizaÃ§Ã£o:
# return normalize_text_advanced(final_text)
return final_text  # SEM normalizaÃ§Ã£o
```

Re-executar e comparar.

---

### AnÃ¡lise do Ground Truth

Verificar quais casos estÃ£o falhando:

```python
# Adicionar em run_diagnostics():
for query, expected in test_cases:
    results = ...
    if not match:
        print(f"âŒ FALHOU: '{query}' esperava '{expected}'")
```

---

## ðŸ“ˆ EXPECTATIVA PÃ“S-CORREÃ‡ÃƒO

### CenÃ¡rio 1: Bug do Cache era a Causa Principal

**Se limpar cache e re-executar:**
- Score esperado: **70-75/100** âœ…
- Top-5: **80-85%**
- Top-1: **55-60%**

**DiferenÃ§a do inicial:** Leve queda devido a ground truth mais rigoroso (esperado)

---

### CenÃ¡rio 2: NormalizaÃ§Ã£o Ã© o Problema

**Se desabilitar normalizaÃ§Ã£o:**
- Score esperado: **75-80/100** âœ…
- Top-5: **85-90%**
- Top-1: **60-65%**

**AÃ§Ã£o:** Ajustar normalizaÃ§Ã£o para ser menos agressiva

---

### CenÃ¡rio 3: Ground Truth Muito DifÃ­cil

**Se score continuar baixo mesmo sem normalizaÃ§Ã£o:**
- Revisar ground truth
- Remover casos impossÃ­veis
- Categorizar casos por dificuldade

---

## ðŸ“ CHECKLIST DE VALIDAÃ‡ÃƒO

ApÃ³s correÃ§Ãµes:

- [ ] Cache limpo
- [ ] Benchmark re-executado
- [ ] Score >= 70/100
- [ ] Top-5 >= 80%
- [ ] Cache mostra nome correto do modelo
- [ ] Resultados comparados com inicial

---

## ðŸ› BUGS CORRIGIDOS NESTE COMMIT

1. âœ… Nome de modelo no cache (crÃ­tico)
2. âœ… Progresso muito verboso (10000 em vez de 1000)
3. âœ… Falta de debug de qual modelo estÃ¡ sendo usado

---

## ðŸ“ž PRÃ“XIMOS PASSOS

1. **UsuÃ¡rio deve executar:**
   ```bash
   git pull
   python clear_cache.py
   python run_definitive_benchmark.py
   ```

2. **Aguardar resultado**

3. **Reportar score final:**
   - Se >= 70: âœ… Problema resolvido
   - Se < 70: âš ï¸ Investigar normalizaÃ§Ã£o

---

**Preparado por:** Claude Code
**Data:** 2025-10-31
**Status:** ðŸ”§ CORREÃ‡Ã•ES APLICADAS - AGUARDANDO VALIDAÃ‡ÃƒO
