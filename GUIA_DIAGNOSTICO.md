# ğŸ” GUIA DE DIAGNÃ“STICO - InvestigaÃ§Ã£o de Performance

**VersÃ£o:** 3.0
**Data:** 2025-10-31
**Para:** Investigar queda de performance de 75.6 para 55.2

---

## ğŸ“Š RESUMO DO PROBLEMA

| MÃ©trica | Benchmark Inicial | Benchmark v2.0 | VariaÃ§Ã£o |
|---------|------------------|----------------|----------|
| Score | **75.6** | **55.2** | **-20.4** (-27%) |
| Top-5 | 87.5% | 55.7% | -31.8% |
| Top-1 | 62.5% | 36.4% | -26.1% |

**Modelo testado:** `intfloat/multilingual-e5-base`

---

## ğŸ› ï¸ FERRAMENTAS DE DIAGNÃ“STICO

### 1. Script de DiagnÃ³stico Completo

```bash
python diagnose.py --all
```

**O que faz:**
- âœ… Verifica integridade do cache
- âœ… Testa impacto da normalizaÃ§Ã£o
- âœ… Analisa dificuldade do ground truth
- âœ… Compara com resultados anteriores

**Testes individuais:**
```bash
python diagnose.py --cache              # Apenas cache
python diagnose.py --normalization      # Apenas normalizaÃ§Ã£o
python diagnose.py --ground-truth       # Apenas ground truth
python diagnose.py --baseline           # Apenas comparaÃ§Ã£o
```

---

## ğŸ” INVESTIGAÃ‡ÃƒO 1: PROBLEMA Ã‰ DO CACHE?

### Como Saber

Execute o diagnÃ³stico:
```bash
python diagnose.py --cache
```

**Sinais de cache corrompido:**
```
âš ï¸  Modelo com nome genÃ©rico: SentenceTransformer (12731 embeddings)
âš ï¸  PossÃ­veis duplicados: 50 grupos
```

### SoluÃ§Ã£o

Se cache estiver corrompido:
```bash
# 1. Limpar cache
python clear_cache.py

# 2. Re-executar benchmark
python run_definitive_benchmark.py --quick

# 3. Comparar resultado
# Se score voltar para ~75: cache era o problema âœ…
# Se score continuar ~55: problema Ã© outro âš ï¸
```

### Teste Manual

Verificar nome do modelo no cache:
```bash
# Deve mostrar nome real do modelo, nÃ£o "SentenceTransformer"
cat cache/embeddings/metadata.json | grep "model"
```

**Esperado:**
```json
"model": "intfloat/multilingual-e5-base"
```

**Corrompido:**
```json
"model": "SentenceTransformer"  âŒ
```

---

## ğŸ“ INVESTIGAÃ‡ÃƒO 2: PROBLEMA Ã‰ DA NORMALIZAÃ‡ÃƒO?

### Como Saber

Execute o diagnÃ³stico:
```bash
python diagnose.py --normalization
```

**Output mostrarÃ¡:**
```
Original:    NCM 0901.11.10: CafÃ© em grÃ£o nÃ£o descafeinado
Normalizado: ncm 0901 11 10 cafe grao nao descafeinado
âš ï¸  Palavras perdidas: em, :, .
```

Se muitas palavras forem perdidas (>50% dos casos), normalizaÃ§Ã£o pode estar agressiva.

### Teste A/B

**Teste 1: COM normalizaÃ§Ã£o (padrÃ£o)**
```bash
python run_definitive_benchmark.py --quick
# Resultado: score1
```

**Teste 2: SEM normalizaÃ§Ã£o**
```bash
python run_definitive_benchmark.py --quick --no-normalize
# Resultado: score2
```

**AnÃ¡lise:**
```python
if score2 > score1 + 5:
    print("NormalizaÃ§Ã£o estÃ¡ PREJUDICANDO!")
    # AÃ§Ã£o: Desabilitar ou suavizar normalizaÃ§Ã£o
elif score2 < score1 - 5:
    print("NormalizaÃ§Ã£o estÃ¡ AJUDANDO!")
    # AÃ§Ã£o: Manter normalizaÃ§Ã£o
else:
    print("NormalizaÃ§Ã£o tem impacto neutro")
    # AÃ§Ã£o: Pode manter ou remover
```

### SoluÃ§Ã£o

Se normalizaÃ§Ã£o for o problema, **desabilitar**:

```python
# Em data_loader.py (linha 292)
# Comentar normalizaÃ§Ã£o:
else:
    # Retorna SEM normalizaÃ§Ã£o
    return texto  # USAR SEMPRE ESTA LINHA
```

Ou permanentemente via ambiente:
```bash
export DISABLE_NORMALIZATION=1
python run_definitive_benchmark.py
```

---

## ğŸ¯ INVESTIGAÃ‡ÃƒO 3: GROUND TRUTH MUITO DIFÃCIL?

### Como Saber

Execute o diagnÃ³stico:
```bash
python diagnose.py --ground-truth
```

**Output:**
```
DistribuiÃ§Ã£o de Dificuldade:
  FÃ¡ceis:  30/88 (34.1%)
  MÃ©dios:  40/88 (45.5%)
  DifÃ­ceis: 18/88 (20.5%)  ğŸ‘ˆ Se >30%, Ã© esperado queda
```

### AnÃ¡lise

**Casos difÃ­ceis que diminuem score:**
```python
# Muito genÃ©ricos
("carro", "8703")              # DifÃ­cil - pode ser vÃ¡rios NCMs
("celular", "8517")            # DifÃ­cil - ambÃ­guo

# Muito especÃ­ficos
("smartphone samsung galaxy s21 128gb", "8517")  # Muito especÃ­fico

# Erros ortogrÃ¡ficos
("telefon celular", "8517")     # Erro de digitaÃ§Ã£o

# Outras lÃ­nguas
("mobile phone", "8517")        # InglÃªs
```

### SoluÃ§Ã£o

**OpÃ§Ã£o 1:** Aceitar que Ã© esperado
- Ground truth mais rigoroso = score mais realista
- Queda de 5-10 pontos Ã© normal
- **Recomendado:** Manter ground truth rigoroso

**OpÃ§Ã£o 2:** Filtrar casos impossÃ­veis
```python
# Em ground_truth_cases.py
# Remover ou comentar casos muito difÃ­ceis

# Casos muito genÃ©ricos (opcional remover)
# ("carro", "8703"),
# ("celular", "8517"),
```

---

## ğŸ“ˆ INVESTIGAÃ‡ÃƒO 4: COMPARAÃ‡ÃƒO COM BASELINE

### Como Saber

Execute:
```bash
python diagnose.py --baseline
```

**Output mostra histÃ³rico:**
```
HistÃ³rico de Performance:
  2025-10-30 17:08: Score 75.6
  2025-10-31 09:49: Score 55.2

MudanÃ§a: -20.4 pontos âš ï¸  REGRESSÃƒO
```

### AnÃ¡lise

**RegressÃ£o detectada:**
- MudanÃ§as recentes no cÃ³digo
- Cache corrompido
- NormalizaÃ§Ã£o nova

**Melhoria detectada:**
- CorreÃ§Ãµes funcionaram
- Ground truth estava fÃ¡cil demais antes

---

## ğŸ¯ FLUXOGRAMA DE DECISÃƒO

```mermaid
graph TD
    A[Score caiu de 75.6 para 55.2] --> B{DiagnÃ³stico completo}
    B --> C[python diagnose.py --all]

    C --> D{Cache corrompido?}
    D -->|Sim| E[Limpar cache]
    E --> F[Re-executar benchmark]
    F --> G{Score voltou?}
    G -->|Sim 70-75| H[âœ… Problema era cache]
    G -->|NÃ£o ~55| I[Investigar normalizaÃ§Ã£o]

    D -->|NÃ£o| I

    I --> J{Teste A/B normalizaÃ§Ã£o}
    J --> K[Com: score1]
    J --> L[Sem: score2]
    L --> M{score2 > score1 + 5?}
    M -->|Sim| N[âœ… Desabilitar normalizaÃ§Ã£o]
    M -->|NÃ£o| O[NormalizaÃ§Ã£o OK]

    O --> P{Ground truth difÃ­cil?}
    P -->|>30% difÃ­cil| Q[âœ… Queda esperada]
    P -->|<30%| R[âš ï¸ Investigar mais]
```

---

## ğŸ“‹ CHECKLIST DE INVESTIGAÃ‡ÃƒO

Use este checklist para investigaÃ§Ã£o sistemÃ¡tica:

- [ ] **1. Executei diagnÃ³stico completo**
  ```bash
  python diagnose.py --all
  ```

- [ ] **2. Verifiquei cache**
  - [ ] Cache mostra nome correto do modelo?
  - [ ] NÃ£o hÃ¡ modelos com "SentenceTransformer"?
  - [ ] Se corrompido, limpei com `clear_cache.py`?

- [ ] **3. Testei normalizaÃ§Ã£o A/B**
  - [ ] Executei com normalizaÃ§Ã£o
  - [ ] Executei sem normalizaÃ§Ã£o (`--no-normalize`)
  - [ ] Comparei scores

- [ ] **4. Analisei ground truth**
  - [ ] Verifiquei distribuiÃ§Ã£o de dificuldade
  - [ ] Casos difÃ­ceis < 30%?
  - [ ] Removi casos impossÃ­veis (opcional)

- [ ] **5. Comparei com baseline**
  - [ ] Revisei histÃ³rico de resultados
  - [ ] Identifiquei quando performance caiu
  - [ ] Correlacionei com mudanÃ§as no cÃ³digo

- [ ] **6. Documentei resultados**
  - [ ] Registrei scores de cada teste
  - [ ] Identifiquei causa principal
  - [ ] Apliquei correÃ§Ã£o

---

## ğŸ”§ COMANDOS ÃšTEIS

### Limpeza e Reset

```bash
# Limpar cache completo
python clear_cache.py

# Limpar apenas modelo especÃ­fico
python clear_cache.py --model e5

# Remover bancos de benchmark antigos
rm -rf benchmark_db_*

# Reset completo
python clear_cache.py && rm -rf benchmark_db_* && rm benchmark_results_*.json
```

### Testes EspecÃ­ficos

```bash
# Teste rÃ¡pido com normalizaÃ§Ã£o
python run_definitive_benchmark.py --quick

# Teste rÃ¡pido SEM normalizaÃ§Ã£o
python run_definitive_benchmark.py --quick --no-normalize

# Teste sem cache
python run_definitive_benchmark.py --quick --no-cache

# Teste completo limpo
python clear_cache.py && python run_definitive_benchmark.py
```

### DiagnÃ³sticos

```bash
# DiagnÃ³stico completo
python diagnose.py --all

# Ver cache
cat cache/embeddings/metadata.json | grep -A 2 "model"

# Listar resultados
ls -lh benchmark_results_*.json

# Ver Ãºltimo resultado
cat $(ls -t benchmark_results_*.json | head -1) | jq '.[] | select(.model_name=="intfloat/multilingual-e5-base") | {score, accuracy_top5, accuracy_top1}'
```

---

## ğŸ“Š RESULTADOS ESPERADOS

### CenÃ¡rio 1: Cache Corrompido (principal suspeito)

**AÃ§Ã£o:** Limpar cache
```bash
python clear_cache.py
python run_definitive_benchmark.py --quick
```

**Resultado esperado:**
- Score: **70-75/100** âœ…
- Top-5: 80-85%
- Top-1: 55-60%

**InterpretaÃ§Ã£o:** Problema resolvido, leve queda devido a ground truth mais rigoroso (esperado)

---

### CenÃ¡rio 2: NormalizaÃ§Ã£o Agressiva

**AÃ§Ã£o:** Desabilitar normalizaÃ§Ã£o
```bash
python run_definitive_benchmark.py --quick --no-normalize
```

**Resultado esperado:**
- Score: **75-80/100** âœ…
- Top-5: 85-90%
- Top-1: 60-65%

**InterpretaÃ§Ã£o:** NormalizaÃ§Ã£o estava removendo informaÃ§Ã£o importante

---

### CenÃ¡rio 3: Ground Truth DifÃ­cil

**AÃ§Ã£o:** Nenhuma (ou filtrar casos)
**Resultado:** Score permanece ~55-60

**InterpretaÃ§Ã£o:** Ground truth realista mostra performance verdadeira. Queda Ã© esperada e saudÃ¡vel.

---

### CenÃ¡rio 4: CombinaÃ§Ã£o de Fatores

**AÃ§Ã£o:** Limpar cache + desabilitar normalizaÃ§Ã£o
```bash
python clear_cache.py
python run_definitive_benchmark.py --quick --no-normalize
```

**Resultado esperado:**
- Score: **75-82/100** âœ…
- MÃºltiplos problemas resolvidos

---

## ğŸ“ LIÃ‡Ã•ES APRENDIDAS

### Sobre Cache

âœ… **Sempre verificar** nome do modelo no cache
âœ… **Limpar cache** apÃ³s mudanÃ§as estruturais
âœ… **NÃ£o confiar** em cache apÃ³s cÃ³digo modificado

### Sobre NormalizaÃ§Ã£o

âš ï¸ **Testar impacto** antes de aplicar
âš ï¸ **Comparar A/B** sempre que modificar
âš ï¸ **Menos pode ser mais** - normalizaÃ§Ã£o agressiva prejudica

### Sobre Ground Truth

ğŸ“Š **Casos realistas** > casos fÃ¡ceis
ğŸ“Š **Aceitar queda** se ground truth melhorou
ğŸ“Š **Categorizar dificuldade** para entender scores

---

## ğŸ“ PRÃ“XIMOS PASSOS

1. **Execute diagnÃ³stico:** `python diagnose.py --all`
2. **Identifique causa principal**
3. **Aplique correÃ§Ã£o apropriada**
4. **Re-execute benchmark:** `python run_definitive_benchmark.py`
5. **Compare resultado** com este guia
6. **Documente** no CHANGELOG

---

**Preparado por:** Claude Code
**Ãšltima atualizaÃ§Ã£o:** 2025-10-31
**VersÃ£o:** 3.0 (Completo com ferramentas)
