#!/usr/bin/env python3
# run_definitive_benchmark.py
# Script para execu√ß√£o definitiva do benchmark com todas as melhorias integradas

"""
BENCHMARK DEFINITIVO DE EMBEDDINGS - RAG NCM

Este script executa o benchmark completo com todas as melhorias:
‚úì Ground truth expandido (90+ casos)
‚úì Cache de embeddings (acelera re-execu√ß√µes)
‚úì Normaliza√ß√£o avan√ßada de textos
‚úì Valida√ß√£o de modelos antes de testar
‚úì Tratamento robusto de erros
‚úì Gera√ß√£o autom√°tica da configura√ß√£o ideal

Uso:
    python run_definitive_benchmark.py [op√ß√µes]

Op√ß√µes:
    --no-cache          Desabilita cache (for√ßa rec√°lculo)
    --quick             Testa apenas modelo base (para valida√ß√£o r√°pida)
    --all               Inclui modelo r√°pido para compara√ß√£o

Resultado:
    - benchmark_results_*.json       : Resultados completos
    - best_model_config.json         : Configura√ß√£o JSON do melhor modelo
    - best_model_config.py           : M√≥dulo Python para import direto
"""

import sys
import os
import argparse
from datetime import datetime

# Adiciona diret√≥rio do projeto ao path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from benchmark_embeddings import EmbeddingBenchmark, MODELS_TO_TEST


def print_banner():
    """Banner inicial"""
    print("\n" + "="*80)
    print("  BENCHMARK DEFINITIVO DE MODELOS DE EMBEDDING - RAG NCM")
    print("="*80)
    print(f"  Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  Vers√£o: 2.0 (com melhorias integradas)")
    print("="*80)


def print_improvements():
    """Lista de melhorias implementadas"""
    print("\nüìä MELHORIAS IMPLEMENTADAS:")
    print("  ‚úì Ground truth expandido: 90+ casos (vs 8 anteriores)")
    print("  ‚úì Cache de embeddings: acelera 80-90% em re-execu√ß√µes")
    print("  ‚úì Normaliza√ß√£o avan√ßada: remove ru√≠do dos textos")
    print("  ‚úì Valida√ß√£o pr√©via: evita erros com modelos inv√°lidos")
    print("  ‚úì Auto-configura√ß√£o: gera setup ideal automaticamente")
    print()


def print_models(models, mode="default"):
    """Lista modelos a serem testados"""
    print(f"\nüî¨ MODELOS SELECIONADOS ({mode}):")
    for i, model in enumerate(models, 1):
        print(f"  {i}. {model}")
    print()


def confirm_execution(models, use_cache):
    """Confirma execu√ß√£o do benchmark"""
    print("\n‚öôÔ∏è  CONFIGURA√á√ÉO:")
    print(f"  Modelos a testar: {len(models)}")
    print(f"  Cache: {'ATIVO ‚úì' if use_cache else 'DESATIVADO'}")
    print(f"  Ground truth: 90+ casos de teste")
    print(f"  Tempo estimado: {'~20-40 min' if use_cache else '~3-5 horas'}")
    print()

    try:
        response = input("Deseja continuar? [S/n]: ").strip().lower()
        if response and response not in ['s', 'sim', 'y', 'yes']:
            print("\n‚ùå Benchmark cancelado pelo usu√°rio")
            return False
    except KeyboardInterrupt:
        print("\n\n‚ùå Benchmark cancelado pelo usu√°rio")
        return False

    return True


def run_benchmark(use_cache=True, models=None):
    """Executa benchmark com configura√ß√µes especificadas"""
    if models is None:
        models = MODELS_TO_TEST

    print(f"\n{'='*80}")
    print("INICIANDO BENCHMARK")
    print(f"{'='*80}\n")

    try:
        benchmark = EmbeddingBenchmark(use_cache=use_cache)
        benchmark.load_data()

        # Testa cada modelo
        for i, model_name in enumerate(models, 1):
            print(f"\n{'='*80}")
            print(f"PROGRESSO: {i}/{len(models)} modelos")
            print(f"{'='*80}")
            benchmark.test_model(model_name)

        # Gera relat√≥rio comparativo
        benchmark.generate_comparative_report()

        print(f"\n{'='*80}")
        print("‚úì BENCHMARK CONCLU√çDO COM SUCESSO!")
        print(f"{'='*80}\n")

        print("üìÅ ARQUIVOS GERADOS:")
        print("  - benchmark_results_*.json    : Resultados detalhados")
        print("  - best_model_config.json      : Configura√ß√£o ideal (JSON)")
        print("  - best_model_config.py        : M√≥dulo Python para import")
        print()

        print("üí° PR√ìXIMOS PASSOS:")
        print("  1. Revisar best_model_config.json")
        print("  2. Se score >= 80: usar modelo em produ√ß√£o")
        print("  3. Se score < 80: implementar re-ranking (ver PLANO_ACAO.md)")
        print("  4. Testar em ambiente real")
        print()

        return True

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Benchmark interrompido pelo usu√°rio")
        print("Resultados parciais podem ter sido salvos")
        return False

    except Exception as e:
        print(f"\n\n‚ùå ERRO durante benchmark: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(
        description="Execu√ß√£o definitiva do benchmark de embeddings",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run_definitive_benchmark.py
  python run_definitive_benchmark.py --no-cache
  python run_definitive_benchmark.py --quick
  python run_definitive_benchmark.py --all

Para mais informa√ß√µes: README.md ou PLANO_ACAO.md
        """
    )

    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='Desabilita cache (for√ßa rec√°lculo de todos embeddings)'
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='Modo r√°pido: testa apenas modelo base (para valida√ß√£o)'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='Testa todos os modelos incluindo o r√°pido (all-MiniLM-L6-v2)'
    )

    parser.add_argument(
        '--yes', '-y',
        action='store_true',
        help='Pula confirma√ß√£o (auto-confirma execu√ß√£o)'
    )

    args = parser.parse_args()

    # Banner
    print_banner()
    print_improvements()

    # Seleciona modelos
    if args.quick:
        models = ['intfloat/multilingual-e5-base']
        mode = "modo r√°pido"
    elif args.all:
        models = MODELS_TO_TEST + ['sentence-transformers/all-MiniLM-L6-v2']
        mode = "todos os modelos"
    else:
        models = MODELS_TO_TEST
        mode = "padr√£o"

    print_models(models, mode)

    # Configura√ß√µes
    use_cache = not args.no_cache

    # Confirma execu√ß√£o
    if not args.yes:
        if not confirm_execution(models, use_cache):
            sys.exit(0)

    # Executa benchmark
    print("\nüöÄ Iniciando benchmark...\n")
    success = run_benchmark(use_cache=use_cache, models=models)

    # Status final
    if success:
        print("\n‚úì Script finalizado com sucesso!")
        sys.exit(0)
    else:
        print("\n‚úó Script finalizado com erros")
        sys.exit(1)


if __name__ == "__main__":
    main()
