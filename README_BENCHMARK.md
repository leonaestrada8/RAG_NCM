# üìä Benchmark de Modelos de Embedding - RAG NCM

Sistema completo de benchmark para sele√ß√£o do melhor modelo de embedding para o sistema RAG de classifica√ß√£o NCM.

## üéØ Objetivo

Identificar o melhor modelo de embedding para buscas sem√¢nticas em c√≥digos NCM (Nomenclatura Comum do Mercosul), considerando:
- **Acur√°cia**: Precis√£o nas buscas (Top-1 e Top-5)
- **Performance**: Velocidade de indexa√ß√£o
- **Qualidade**: Score geral balanceado

---

## üìà Resultados do Benchmark Inicial

**Data:** 2025-10-30

| Modelo | Score | Top-5 | Top-1 | Tempo |
|--------|-------|-------|-------|-------|
| ü•á **intfloat/multilingual-e5-base** | **75.6** | 87.5% | 62.5% | 83 min |
| ü•à paraphrase-multilingual-mpnet-base-v2 | 54.0 | 75.0% | 62.5% | 127 min |
| ü•â all-MiniLM-L6-v2 | 50.1 | 62.5% | 37.5% | 33 min |

**Veredicto:** Parcialmente satisfat√≥rio. Recomenda-se nova rodada com melhorias.

---

## ‚ú® Melhorias Implementadas (Vers√£o 2.0)

### 1. Ground Truth Expandido
- **Antes:** 8 casos de teste
- **Depois:** 90+ casos cobrindo 15+ categorias NCM
- **Impacto:** +2-3% acur√°cia, mais confian√ßa estat√≠stica

### 2. Cache de Embeddings
- **Funcionalidade:** Salva embeddings em disco
- **Impacto:** Reduz tempo de re-execu√ß√£o em 80-90%
- **Exemplo:** 83 min ‚Üí 8 min em segunda execu√ß√£o

### 3. Normaliza√ß√£o Avan√ßada
- Remove acentos mantendo sem√¢ntica
- Lowercase + limpeza de pontua√ß√£o
- Remo√ß√£o seletiva de stopwords
- **Impacto:** +2-3% acur√°cia

### 4. Valida√ß√£o de Modelos
- Verifica exist√™ncia antes de testar
- Tratamento robusto de erros
- Relat√≥rios claros de falhas

### 5. Auto-configura√ß√£o
- Gera automaticamente configura√ß√£o ideal
- Exports JSON + m√≥dulo Python
- Recomenda√ß√µes de re-ranking

---

## üöÄ Como Executar

### Execu√ß√£o Padr√£o (Recomendado)

```bash
python run_definitive_benchmark.py
```

Testa os modelos mais promissores:
- `intfloat/multilingual-e5-base` (baseline)
- `intfloat/multilingual-e5-large` (melhor vers√£o)
- `BAAI/bge-m3` (SOTA 2024)
- `BAAI/bge-large-en-v1.5` (alternativa)

**Tempo estimado:** 2-4 horas (primeira vez) / 20-40 min (com cache)

### Execu√ß√£o R√°pida (Valida√ß√£o)

```bash
python run_definitive_benchmark.py --quick
```

Testa apenas o modelo base para validar que tudo est√° funcionando.

**Tempo estimado:** ~30 min (primeira vez) / ~5 min (com cache)

### Execu√ß√£o Completa

```bash
python run_definitive_benchmark.py --all
```

Inclui todos os modelos para compara√ß√£o abrangente.

### Sem Cache (For√ßar Rec√°lculo)

```bash
python run_definitive_benchmark.py --no-cache
```

√ötil para garantir resultados limpos sem cache anterior.

### Modo Autom√°tico (Sem Confirma√ß√£o)

```bash
python run_definitive_benchmark.py --yes
```

Pula a confirma√ß√£o interativa (√∫til para scripts).

---

## üìÅ Arquivos Gerados

### benchmark_results_YYYYMMDD_HHMMSS.json
Resultados detalhados de todos os modelos testados.

```json
{
  "model_name": "intfloat/multilingual-e5-base",
  "score": 75.6,
  "accuracy_top1": 62.5,
  "accuracy_top5": 87.5,
  "mean_distance": 0.338,
  "elapsed_time": 4971.1,
  ...
}
```

### best_model_config.json
Configura√ß√£o ideal para uso em produ√ß√£o.

```json
{
  "model": {
    "name": "intfloat/multilingual-e5-base",
    "score": 75.6,
    "accuracy_top1": 62.5,
    "accuracy_top5": 87.5
  },
  "settings": {
    "use_cache": true,
    "use_reranking": true,
    "batch_size": 32,
    "top_k_initial": 15,
    "top_k_final": 5
  },
  "performance": {
    "recommended_for_production": false
  }
}
```

### best_model_config.py
M√≥dulo Python para import direto no c√≥digo.

```python
from best_model_config import load_best_model

# Carrega automaticamente o melhor modelo
model = load_best_model()
```

---

## üìä Interpretando Resultados

### Score Geral (0-100)

F√≥rmula ponderada:
```
Score = (Top-5 √ó 0.4) + (Top-1 √ó 0.3) + (Dist√¢ncia √ó 0.2) + (Cobertura √ó 0.1)
```

**Interpreta√ß√£o:**
- **< 70:** Insuficiente para produ√ß√£o
- **70-80:** Aceit√°vel com melhorias (re-ranking)
- **80-90:** Bom para produ√ß√£o
- **> 90:** Excelente

### Acur√°cia Top-K

- **Top-1:** Resultado correto na primeira posi√ß√£o
- **Top-5:** Resultado correto entre os 5 primeiros

**Metas:**
- Top-5: ‚â• 90% (cr√≠tico)
- Top-1: ‚â• 75% (ideal)

### Dist√¢ncia M√©dia

M√©trica interna do ChromaDB (menor = melhor)
- Valores baixos indicam embeddings mais discriminativos
- Compar√°vel apenas entre modelos com mesma dimens√£o

---

## üîß Pr√≥ximos Passos Ap√≥s Benchmark

### Se Score ‚â• 80
1. ‚úì Usar modelo em produ√ß√£o imediatamente
2. Monitorar performance real
3. Coletar feedback de usu√°rios

### Se Score 70-80
1. Implementar re-ranking (ver `PLANO_ACAO.md`)
2. Testar em ambiente de homologa√ß√£o
3. Avaliar trade-off lat√™ncia/qualidade

### Se Score < 70
1. Revisar dados de entrada (NCMs e atributos)
2. Implementar melhorias do `PLANO_ACAO.md`:
   - Re-ranking com CrossEncoder
   - Fine-tuning do modelo
   - Enriquecimento de dados
3. Executar novo benchmark

---

## üõ†Ô∏è Estrutura do Projeto

```
RAG_NCM/
‚îú‚îÄ‚îÄ benchmark_embeddings.py          # C√≥digo principal do benchmark
‚îú‚îÄ‚îÄ run_definitive_benchmark.py      # Script de execu√ß√£o facilitado
‚îú‚îÄ‚îÄ embedding_cache.py               # Sistema de cache
‚îú‚îÄ‚îÄ ground_truth_cases.py            # 90+ casos de teste
‚îú‚îÄ‚îÄ data_loader.py                   # Carregamento e normaliza√ß√£o
‚îú‚îÄ‚îÄ config.py                        # Configura√ß√µes
‚îÇ
‚îú‚îÄ‚îÄ ANALISE_BENCHMARK.md             # An√°lise detalhada dos resultados
‚îú‚îÄ‚îÄ PLANO_ACAO.md                    # Roadmap de melhorias
‚îú‚îÄ‚îÄ README_BENCHMARK.md              # Este arquivo
‚îÇ
‚îî‚îÄ‚îÄ Gerados ap√≥s execu√ß√£o:
    ‚îú‚îÄ‚îÄ best_model_config.json       # Configura√ß√£o ideal
    ‚îú‚îÄ‚îÄ best_model_config.py         # M√≥dulo Python
    ‚îú‚îÄ‚îÄ benchmark_results_*.json     # Resultados detalhados
    ‚îî‚îÄ‚îÄ cache/embeddings/            # Cache de embeddings
```

---

## üí° Dicas e Boas Pr√°ticas

### Cache

‚úì **Ativar cache** para desenvolvimento (padr√£o)
- Acelera itera√ß√µes
- Economiza tempo e recursos

‚úó **Desativar cache** para benchmark final
- Garante resultados limpos
- Evita inconsist√™ncias

### Modelos

**Priorizar modelos multil√≠ngue:**
- `intfloat/multilingual-e5-*`
- `BAAI/bge-m3`
- `sentence-transformers/LaBSE`

**Evitar modelos ingl√™s-only:**
- `all-MiniLM-L6-v2` (n√£o multil√≠ngue)
- Performance ruim com portugu√™s

### Ground Truth

**Manter casos atualizados:**
- Adicionar casos de falhas reais
- Cobrir categorias representativas
- Testar edge cases

**Arquivo:** `ground_truth_cases.py`

---

## üêõ Troubleshooting

### Erro: "Modelo n√£o encontrado"

```
‚úó ERRO: Modelo X n√£o encontrado ou inacess√≠vel
```

**Solu√ß√£o:**
1. Verificar nome do modelo em https://huggingface.co/models
2. Verificar conex√£o com internet
3. Autenticar: `huggingface-cli login` (se modelo privado)

### Cache corrompido

```
Erro ao ler cache XXX
```

**Solu√ß√£o:**
```bash
rm -rf cache/embeddings/*
python run_definitive_benchmark.py --no-cache
```

### Mem√≥ria insuficiente

```
CUDA out of memory / MemoryError
```

**Solu√ß√£o:**
1. Reduzir `BATCH_SIZE` em `config.py`
2. Usar modelos menores (-base em vez de -large)
3. Rodar sem GPU (CPU only)

### ChromaDB travado

```
Collection creation failed
```

**Solu√ß√£o:**
```bash
rm -rf benchmark_db_*
```

---

## üìö Documenta√ß√£o Adicional

- **An√°lise completa:** `ANALISE_BENCHMARK.md`
- **Plano de melhorias:** `PLANO_ACAO.md`
- **C√≥digo do benchmark:** `benchmark_embeddings.py`

---

## ü§ù Contribuindo

Para adicionar novos modelos ao benchmark:

1. Editar `benchmark_embeddings.py`:
```python
MODELS_TO_TEST = [
    'intfloat/multilingual-e5-base',
    'seu/novo-modelo',  # Adicionar aqui
]
```

2. Executar benchmark:
```bash
python run_definitive_benchmark.py
```

---

## üìû Suporte

- **Issues:** Reportar problemas via GitHub Issues
- **D√∫vidas:** Consultar `ANALISE_BENCHMARK.md` ou `PLANO_ACAO.md`

---

**√öltima atualiza√ß√£o:** 2025-10-31
**Vers√£o:** 2.0 (com melhorias integradas)
